# CACIS — Cost-Aware Classification with Informative Selection
## Mathematical Foundations

This document provides a technical specification of the **CACIS** framework, bridging the gap between decision theory and deep learning.

---

## 1. Problem Setting: Misclassification Costs

We address supervised learning problems where classification errors have asymmetric, instance-dependent costs. We consider i.i.d. triples $(x, C, y) \sim \mathcal{D}$ where:

- $x \in \mathcal{X}$ is the input feature vector.
- $y \in \{1, \dots, K\}$ is the ground truth class.
- $C \in \mathbb{R}_+^{K \times K}$ is the **misclassification cost matrix**.
    - $c_{y,k}$ is the cost of predicting class $k$ when the true label is $y$.
    - We assume $c_{y,y} = 0$.

The goal is to find a model $f$ such that the derived decision $k(x)$ minimizes the **expected risk**:
```math
R(k) = \mathbb{E}_{(X,C,Y)} [ c_{Y, k(X)}(C) ].
```

---

## 2. CACIS as a Fenchel–Young Loss

CACIS (Cost-Aware Classification with Implicit Scores) is a **Fenchel–Young loss** $\ell_\Omega$ generated by a specific structure-aware regularizer $\Omega$.

For a strictly convex regularizer $\Omega : \Delta_K \to \mathbb{R}$, the Fenchel–Young loss is:
```math
\ell_\Omega(y, f) = \Omega^*(f) - f_y + \Omega(\delta_y),
```
where $\Omega^*$ is the **Fenchel conjugate** of $\Omega$, defined on the score space $\mathbb{R}^K$:
```math
\Omega^*(f) = \sup_{\alpha \in \Delta_K} \left( \langle \alpha, f \rangle - \Omega(\alpha) \right).
```

### Gradients and Predicted Probabilities
By Danskin's Theorem, the gradient of $\Omega^*$ yields the predicted probability distribution:
```math
\nabla_f \Omega^*(f) = \arg\max_{\alpha \in \Delta_K} \left( \langle \alpha, f \rangle - \Omega(\alpha) \right) = q(y \mid x).
```
The gradient of the loss $\ell_\Omega$ is simply the prediction error:
```math
\nabla_f \ell_\Omega(y, f) = q(y \mid x) - \delta_y.
```

---

## 3. The CACIS Regularizer: Sinkhorn Negentropy

Unlike standard Cross-Entropy which uses Shannon entropy (isotropic), CACIS uses the **Sinkhorn Negentropy** $\Omega_{C,\varepsilon}$ based on entropic Optimal Transport:

```math
\Omega_{C,\varepsilon}(\alpha) = - \frac{1}{2} \mathrm{OT}_{C,\varepsilon}(\alpha, \alpha).
```

### The Variational Form of the Conjugate
Deriving $\Omega^*$ for this specific regularizer (based on the work of Mensch et al., 2019) leads to a log-partition function over the simplex:

```math
\Omega_{C,\varepsilon}^*(f) = - \varepsilon \log \left( \min_{\alpha \in \Delta_K} \alpha^\top M(f, C, \varepsilon) \alpha \right),
```
where $M$ is a $K \times K$ matrix with entries:
```math
M_{ij} = \exp\left( - \frac{f_i + f_j + c_{ij}}{\varepsilon} \right).
```

---

## 4. Optimization: Frank–Wolfe on the Simplex

To evaluate the loss and its gradients, we must solve the inner optimization problem:
```math
\min_{\alpha \in \Delta_K} \mathcal{G}(\alpha) \quad \text{where} \quad \mathcal{G}(\alpha) = \alpha^\top M \alpha.
```

We use the **Frank–Wolfe (Conditional Gradient)** algorithm:
1. Initialize $\alpha^{(0)}$ to the uniform distribution $\mathbf{1}/K$.
2. For $t = 0$ to $T$:
    a. Compute gradient: $\nabla \mathcal{G}(\alpha^{(t)}) = 2 M \alpha^{(t)}$.
    b. Linear oracle: $s = \arg\min_{s \in \Delta_K} \langle s, \nabla \mathcal{G}(\alpha^{(t)}) \rangle$ (this is a one-hot vector at the index of the minimum gradient entry).
    c. Update: $\alpha^{(t+1)} = (1 - \gamma_t) \alpha^{(t)} + \gamma_t s$, where $\gamma_t = 2 / (t+2)$.

### Implicit Gradients
Crucially, we do **not** backpropagate through the Frank–Wolfe iterations. Instead, we use the property that $\nabla \Omega^*(f)$ is defined by the optimal $\alpha^*$ found by the solver. This is efficient and numerically stable.

---

## 5. Interpretability: Baseline Normalization

Raw CACIS loss values (in units of cost) can be difficult to monitor across different datasets. We define a **Baseline-Normalized CACIS Loss** to provide a "0 to 1" scale.

### The Uninformed Baseline
The baseline $f_{\text{base}}$ corresponds to the decision maker who knows the average cost structure $C$ but not the specific features $x$. The baseline scores are typically the negative expected costs:
```math
f_{\text{base}, k} = - \mathbb{E}_y [ c_{y, k} ].
```

### Normalized Score
The normalized loss for an example $i$ is:
```math
\ell_{\text{norm}, i} = \frac{\ell_{\text{raw}}(y_i, f_i; C_i)}{\ell_{\text{raw}}(y_i, f_{\text{base}}; C_i)}.
```

**Interpretation:**
- **0.0**: Perfect prediction (Zero cost).
- **1.0**: Performance of the uninformed cost-aware baseline.
- **< 1.0**: Improvement over the baseline (The model is extracting useful information from $x$).

---

## 6. Scientific Origins

CACIS is an implementation of **Geometric Losses for Distributional Learning** applied to cost-sensitive classification.

**Key Reference:**
> Arthur Mensch, Mathieu Blondel, Gabriel Peyré. **Geometric Losses for Distributional Learning**. *arXiv preprint arXiv:1905.06005*, 2019.

---

## 7. Numerical Stability and Invariances

1. **Shift Invariance**: Adding a constant to all scores $f \mapsto f + a\mathbf{1}$ does not change the predicted $\alpha^*$.
2. **Cost Rescaling**: Multiplying costs $C$ and temperature $\varepsilon$ by a factor $\sigma > 0$ preserves the optimal $\alpha^*$.
3. **Log-sum-exp Trick**: In practice, we compute the log of the quadratic form to avoid underflow associated with the exponential entries in $M$.
