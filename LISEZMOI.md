# CACIS ‚Äî Cost-Aware Classification with Informative Selection

[![Licence](https://img.shields.io/badge/License-BSD%203--Clause-blue.svg)](https://opensource.org/licenses/BSD-3-Clause)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch 2.0+](https://img.shields.io/badge/pytorch-2.0+-ee4c2c.svg)](https://pytorch.org/)

**CACIS** est un framework open source de **classification d√©cisionnelle** (th√©orie de la d√©cision) avec des **co√ªts de mauvaise classification d√©pendant de l‚Äôexemple**, exprim√©s en unit√©s du monde r√©el (euros, √©nergie, temps, risque).

> **La pr√©cision n‚Äôest pas l‚Äôobjectif ‚Äî ce sont les d√©cisions et leurs co√ªts.**

---

## üí° Motivation

Dans les syst√®mes d‚ÄôIA du monde r√©el (sant√©, finance, robotique) :
- **Les co√ªts de mauvaise classification varient selon l‚Äôexemple** : un faux n√©gatif sur une maladie rare co√ªte plus cher qu‚Äôun faux n√©gatif sur un simple rhume.
- **Les co√ªts sont dynamiques** : ils peuvent n‚Äô√™tre connus qu‚Äôau moment de la d√©cision (p. ex. prix de march√©).
- **L‚Äôaccuracy n‚Äôest qu‚Äôun proxy** : les m√©triques standards capturent mal l‚Äôimpact business ou s√©curit√© d‚Äôune d√©cision.

Les pertes standard comme l‚Äôentropie crois√©e (Cross-Entropy) supposent des co√ªts uniformes. CACIS fournit une alternative de principe, d√©riv√©e du **transport optimal** et des **pertes de Fenchel‚ÄìYoung**.

---

## üß† Fondations scientifiques

CACIS s‚Äôappuie sur la th√©orie de [*Geometric Losses for Distributional Learning*](https://arxiv.org/abs/1905.06005). Il exploite le transport optimal entropique (Sinkhorn) pour ¬´ fa√ßonner ¬ª le simplexe des probabilit√©s selon la g√©om√©trie des co√ªts.

En r√©gularisant l‚Äôapprentissage avec une n√©gentropie de Sinkhorn sensible aux co√ªts, CACIS garantit que le mod√®le apprend une distribution naturellement ¬´ tordue ¬ª vers des d√©cisions co√ªt-efficaces.

Vous pouvez lire les d√©tails math√©matiques de CACIS dans le fichier [math.md](math.md).

---

## üöÄ D√©marrage rapide

### Installation

Veuillez d‚Äôabord lire les [instructions d‚Äôinstallation conda](https://harchaoui.org/warith/4ml)

```bash
# Cloner le d√©p√¥t
git clone https://github.com/warith-harchaoui/cacis.git
cd cacis

# Installer le package
pip install -e .
```

### Utilisation de base

```python
import torch
from cacis import CACISLoss

# Scores du mod√®le (B, K)
logits = torch.randn(8, 10, requires_grad=True)
# Labels de v√©rit√© terrain (B,)
labels = torch.randint(0, 10, (8,))
# Matrices de co√ªts d√©pendantes de l‚Äôexemple (B, K, K)
costs = torch.rand(8, 10, 10)

criterion = CACISLoss()
# Renvoie (raw_loss, normalized_loss, is_normalized)
output = criterion(logits, labels, C=costs)
loss = output.loss
loss.backward()
```

---

## ü™© D√©mos mises en avant

### 1. ResNet sur CIFAR-10 (co√ªts s√©mantiques)

Nous utilisons des embeddings s√©mantiques **fastText** pour d√©finir des co√ªts sur CIFAR-10. Cette d√©mo montre comment les erreurs entre classes ¬´ proches ¬ª (p. ex. *Chat* vs *Chien*) sont moins p√©nalis√©es que les erreurs entre classes ¬´ √©loign√©es ¬ª (p. ex. *Chat* vs *Camion*), gr√¢ce aux similarit√©s s√©mantiques fastText.

```bash
# Ex√©cution standard
python image_classification.py

# Ex√©cution avec reporting CACIS normalis√© (plus lent) pour une meilleure interpr√©tation
python image_classification.py --normalization
```

![Trajectoire de la perte](assets/image_loss_trajectory.png)

### 2. D√©tection de fraude IEEE-CIS (co√ªts √©conomiques)

Une d√©mo tabulaire sur le dataset Kaggle *IEEE-CIS Fraud Detection*, o√π les co√ªts sont directement proportionnels aux montants des transactions.

T√©l√©charger le dataset Kaggle IEEE-CIS Fraud Detection :
```bash
mkdir ieee-fraud-detection
wget -c http://deraison.ai/ai/ieee-fraud-detection.zip
unzip ieee-fraud-detection.zip -d ieee-fraud-detection
```

Utilisation :
```bash
python fraud_detection.py
```

![Trajectoire de la perte](assets/fraud_loss_trajectory.png)

---

## üìÇ Structure du projet

```text
cacis/
‚îú‚îÄ‚îÄ cacis/                  # Package principal
‚îÇ   ‚îú‚îÄ‚îÄ nn/                 # Sous-modules r√©seaux de neurones
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ loss.py             # Impl√©mentation de CACISLoss
‚îÇ   ‚îú‚îÄ‚îÄ utils.py            # Utilitaires partag√©s (logging, devices, plotting)
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py         # API publique (CACISLoss, utils)
‚îú‚îÄ‚îÄ image_classification.py # D√©mo de classification d‚Äôimages
‚îú‚îÄ‚îÄ fraud_detection.py      # D√©mo de d√©tection de fraude
‚îú‚îÄ‚îÄ tests/                  # Tests unitaires
‚îú‚îÄ‚îÄ math.md                 # D√©rivations math√©matiques (approfondissement)
‚îú‚îÄ‚îÄ setup.py                # Configuration du package
‚îî‚îÄ‚îÄ requirements.txt        # D√©pendances
```

---

## üó∫Ô∏è Feuille de route

- [x] Formulation math√©matique & d√©rivation Fenchel‚ÄìYoung
- [x] Impl√©mentation PyTorch de `CACISLoss`
- [x] Exemples d‚Äôentra√Ænement complets (CIFAR-10 / fastText)
- [ ] `CACISClassifier` compatible scikit-learn
- [ ] Incertitude conforme sensible aux co√ªts
- [ ] Package installable via pip sur PyPI
- [ ] Rapport technique / livre blanc

---

## üìö R√©f√©rences

Si vous utilisez CACIS dans vos travaux, merci de citer :

> Arthur Mensch, Mathieu Blondel, Gabriel Peyr√©. **Geometric Losses for Distributional Learning**. *arXiv preprint arXiv:1905.06005*, 2019. [[Paper]](https://arxiv.org/abs/1905.06005)

```bibtex
@article{mensch2019geometric,
  title={Geometric Losses for Distributional Learning},
  author={Mensch, Arthur and Blondel, Mathieu and Peyr{\'e}, Gabriel},
  journal={arXiv preprint arXiv:1905.06005},
  year={2019}
}
```

---

## ‚öñÔ∏è Licence

Licence BSD 3-Clause. Voir [LICENSE](LICENSE) pour plus de d√©tails.
